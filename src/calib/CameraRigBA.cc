#include "CameraRigBA.h"

#include <boost/unordered_set.hpp>
#include <glibmm.h>
#include <opencv2/core/eigen.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>

#include "ceres/ceres.h"
#include "ceres/covariance.h"
#include "../camera_models/CostFunctionFactory.h"
#include "../features2d/SurfGPU.h"
#include "../gpl/EigenQuaternionParameterization.h"
#include "../gpl/EigenUtils.h"
#include "../npoint/five-point/five-point.hpp"
#include "../visual_odometry/SlidingWindowBA.h"

#ifdef VCHARGE_VIZ
#include "../../../../library/gpl/CameraEnums.h"
#include "../../../../visualization/overlay/GLOverlayExtended.h"
#endif

namespace camodocal
{

CameraRigBA::CameraRigBA(const std::vector<CameraPtr>& cameras,
                         SparseGraph& graph,
                         CameraRigExtrinsics& extrinsics)
 : mCameras(cameras)
 , mExtrinsics(extrinsics)
 , mGraph(graph)
 , kLocalMapWindowDistance(3.0)
 , kMaxDistanceRatio(0.7f)
 , kMaxPoint3DDistance(20.0)
 , kMaxReprojErr(2.0)
 , kMinLoopCorrespondences2D2D(50)
 , kMinInterCorrespondences2D2D(10)
 , kNearestImageMatches(15)
 , kNominalFocalLength(300.0)
 , mVerbose(false)
{

}

void
CameraRigBA::run(int beginStage, bool findLoopClosures, bool optimizeIntrinsics,
                 bool saveWorkingData, std::string dataDir)
{
    // stage 1 - triangulate 3D points with feature correspondences from mono VO and run BA
    // stage 2 - find local inter-camera 3D-3D correspondences and run BA
    // stage 3 - find loop closures and run BA

    if (mVerbose)
    {
        for (size_t i = 0; i < mCameras.size(); ++i)
        {
            std::cout << "# INFO: Camera " << i << ":" << std::endl;
            std::cout << "# INFO:   # segments = " << mGraph.frameSegments(i).size() << std::endl;
            for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
            {
                FrameSegment& segment = mGraph.frameSegments(i).at(j);

                std::cout << "# INFO:   Segment " << j << ": # frames = " << segment.size() << std::endl;
            }
        }
    }

    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
        {
            FrameSegment& segment = mGraph.frameSegments(i).at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                FramePtr& frame = segment.at(k);

                frame->cameraId() = i;
            }
        }
    }

#ifdef VCHARGE_VIZ
    // visualize graph produced by previous stage
    if (beginStage == 1)
    {
        visualize("unopt-", ODOMETRY);
        visualizeExtrinsics("unopt-extrinsics");
    }
    if (beginStage == 2)
    {
        visualize("opt1-BA-", ODOMETRY);
        visualizeExtrinsics("opt1-extrinsics");
    }
    else if (beginStage == 3)
    {
        visualize("opt2-BA-", ODOMETRY);
        visualizeExtrinsics("opt2-extrinsics");
    }
    else if (beginStage > 3)
    {
        visualize("opt3-BA-", ODOMETRY);
        visualizeExtrinsics("opt3-extrinsics");
    }

//    mExtrinsics.readFromFile("../config/calib/calib_camera_kermit/2012_09_03/extrinsic.txt");
//    visualizeExtrinsics("ref-extrinsics");

#endif

    // stage 1
    if (beginStage <= 1)
    {
        prune(PRUNE_BEHIND_CAMERA, CAMERA);

        if (mVerbose)
        {
            std::cout << "# INFO: Triangulating feature correspondences... " << std::endl;
        }

        // remove 3D points generated by monocular VO
        for (size_t i = 0; i < mCameras.size(); ++i)
        {
            for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
            {
                FrameSegment& segment = mGraph.frameSegments(i).at(j);

                for (size_t k = 0; k < segment.size(); ++k)
                {
                    FramePtr& frame = segment.at(k);

                    std::vector<Point2DFeaturePtr>& features2D = frame->features2D();

                    for (size_t l = 0; l < features2D.size(); ++l)
                    {
                        Point2DFeaturePtr& pf = features2D.at(l);

                        if (pf->feature3D().get() != 0)
                        {
                            pf->feature3D() = Point3DFeaturePtr();
                        }
                    }

                    frame->features3D().clear();
                }
            }
        }

        // triangulate feature correspondences to get 3D scene points in odometry frame
        for (size_t i = 0; i < mCameras.size(); ++i)
        {
            Pose T_cam_odo(mExtrinsics.getGlobalCameraPose(i));

            for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
            {
                FrameSegment& segment = mGraph.frameSegments(i).at(j);

                if (segment.size() < 3)
                {
                    continue;
                }

                for (size_t k = 2; k < segment.size(); ++k)
                {
                    triangulateFeatures(segment.at(k-2), segment.at(k-1), segment.at(k),
                                        mCameras.at(i), T_cam_odo);
                }
            }
        }

        if (!validateGraph())
        {
            std::cout << "# ERROR: Graph is not valid." << std::endl;
            exit(1);
        }

        prune(PRUNE_BEHIND_CAMERA, ODOMETRY);

        double minError, maxError, avgError;
        size_t featureCount;

        reprojectionError(minError, maxError, avgError, featureCount, ODOMETRY);

        if (mVerbose)
        {
            std::cout << "# INFO: Reprojection error after triangulation: avg = " << avgError
                      << " px | max = " << maxError << " px | count = " << featureCount << std::endl;
        }

#ifdef VCHARGE_VIZ
        visualize("unopt-", ODOMETRY);
#endif

        if (mVerbose)
        {
            std::cout << "# INFO: Running BA on odometry data... " << std::endl;
        }

        // optimize camera extrinsics and 3D scene points
        optimize(CAMERA_ODOMETRY_EXTRINSICS | POINT_3D, false);

        prune(PRUNE_BEHIND_CAMERA); // | PRUNE_FARAWAY | PRUNE_HIGH_REPROJ_ERR, ODOMETRY);

        reprojectionError(minError, maxError, avgError, featureCount, ODOMETRY);

        if (mVerbose)
        {
            std::cout << "# INFO: Done." << std::endl;
            std::cout << "# INFO: Reprojection error after BA (odometry): avg = " << avgError
                      << " px | max = " << maxError << " px | count = " << featureCount << std::endl;
        }

#ifdef VCHARGE_VIZ
        visualize("opt1-BA-", ODOMETRY);

        visualizeExtrinsics("opt1-extrinsics");
#endif

        if (saveWorkingData)
        {
            boost::filesystem::path extrinsicPath(dataDir);
            extrinsicPath /= "tmp_extrinsic_1.txt";
            mExtrinsics.writeToFile(extrinsicPath.string());

            boost::filesystem::path graphPath(dataDir);
            graphPath /= "frames_1.sg";
            mGraph.writeToBinaryFile(graphPath.string());
        }
    }

    // stage 2
    if (beginStage <= 2)
    {
        if (mVerbose)
        {
            std::cout << "# INFO: Finding inter-map 3D-3D correspondences... " << std::endl;
        }

        prune(PRUNE_BEHIND_CAMERA);

        // find local inter-map 3D-3D correspondences
        std::vector<Correspondence2D2D> localInterMap2D2D;
        findLocalInterMap2D2DCorrespondences(localInterMap2D2D);

        if (mVerbose)
        {
            std::cout << "# INFO: # local inter-map 3D-3D correspondences = "
                      << localInterMap2D2D.size() << std::endl;

            size_t featureCount[mCameras.size()];
            for (size_t i = 0; i < mCameras.size(); ++i)
            {
                featureCount[i] = 0;
            }

            for (size_t i = 0; i < localInterMap2D2D.size(); ++i)
            {
                ++featureCount[localInterMap2D2D.at(i).first->frame()->cameraId()];
                ++featureCount[localInterMap2D2D.at(i).second->frame()->cameraId()];
            }

            for (int i = 0; i < mCameras.size(); ++i)
            {
                std::cout << "# INFO: # features seen in camera " << i
                          << ": " << featureCount[i] << std::endl;
            }
        }

        std::vector<boost::tuple<int, int, FramePtr, FramePtr> > localInterMapFrameFrame;
        for (size_t i = 0; i < localInterMap2D2D.size(); ++i)
        {
            FramePtr& f1 = localInterMap2D2D.at(i).first->frame();
            FramePtr& f2 = localInterMap2D2D.at(i).second->frame();

            localInterMapFrameFrame.push_back(boost::make_tuple(f1->cameraId(), f2->cameraId(), f1, f2));
        }

#ifdef VCHARGE_VIZ
        visualizeFrameFrameCorrespondences("local-inter-p-p", localInterMapFrameFrame);
        visualize3D3DCorrespondences("local-inter-3d-3d", localInterMap2D2D);
#endif

        for (size_t i = 0; i < localInterMap2D2D.size(); ++i)
        {
            Point3DFeaturePtr p1 = localInterMap2D2D.at(i).first->feature3D();
            Point3DFeaturePtr p2 = localInterMap2D2D.at(i).second->feature3D();

            p2->features2D().insert(p2->features2D().end(), p1->features2D().begin(), p1->features2D().end());
            std::sort(p2->features2D().begin(), p2->features2D().end());
            p2->features2D().erase(std::unique(p2->features2D().begin(), p2->features2D().end()), p2->features2D().end());

            std::vector<Point2DFeaturePtr> features2D = p2->features2D();
            for (size_t j = 0; j < features2D.size(); ++j)
            {
                features2D.at(j)->feature3D() = p2;
            }
        }

        prune(PRUNE_BEHIND_CAMERA);

        double minError, maxError, avgError;
        size_t featureCount;

        reprojectionError(minError, maxError, avgError, featureCount, ODOMETRY);

        if (mVerbose)
        {
            std::cout << "# INFO: Done." << std::endl;
            std::cout << "# INFO: Reprojection error with points seen in at least 2 cameras: avg = " << avgError
                      << " px | max = " << maxError << " px | count = " << featureCount << std::endl;
        }

        if (!findLoopClosures)
        {
            std::cout << "# INFO: Running BA on odometry data..." << std::endl;

            if (optimizeIntrinsics)
            {
                // perform BA to optimize intrinsics, extrinsics, odometry poses, and scene points
                optimize(CAMERA_INTRINSICS | CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D, true);
            }
            else
            {
                // perform BA to optimize extrinsics, odometry poses, and scene points
                optimize(CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D, true);
            }

            reprojectionError(minError, maxError, avgError, featureCount, ODOMETRY);

            prune(PRUNE_BEHIND_CAMERA | PRUNE_FARAWAY | PRUNE_HIGH_REPROJ_ERR);

            if (mVerbose)
            {
                std::cout << "# INFO: Done." << std::endl;
                std::cout << "# INFO: Reprojection error after inter-map linking: avg = " << avgError
                          << " px | max = " << maxError << " px | count = " << featureCount << std::endl;
            }

            std::cout << "# INFO: Done." << std::endl;
        }

#ifdef VCHARGE_VIZ
        visualize("opt2-BA-", ODOMETRY);

        visualizeExtrinsics("opt2-extrinsics");
#endif

        if (saveWorkingData)
        {
            if (!findLoopClosures)
            {
                for (int i = 0; i < mCameras.size(); ++i)
                {
                    std::ostringstream oss;
                    oss << "tmp_intrinsic_" << i << ".yaml";

                    boost::filesystem::path intrinsicPath(dataDir);
                    intrinsicPath /= oss.str();
                    mCameras.at(i)->writeParameters(intrinsicPath.string());
                }
            }

            boost::filesystem::path extrinsicPath(dataDir);
            extrinsicPath /= "tmp_extrinsic_2.txt";
            mExtrinsics.writeToFile(extrinsicPath.string());

            boost::filesystem::path graphPath(dataDir);
            graphPath /= "frames_2.sg";
            mGraph.writeToBinaryFile(graphPath.string());
        }
    }

    if (findLoopClosures && beginStage <= 3)
    {
        buildVocTree();

        if (mVerbose)
        {
            std::cout << "# INFO: Finding loop closures... " << std::endl;
        }

        std::vector<boost::tuple<int, int, FramePtr, FramePtr> > loopClosureFrameFrame;
        std::vector<Correspondence3D3D> loopClosure3D3D;
        findLoopClosure3D3D(loopClosureFrameFrame, loopClosure3D3D);

        for (size_t i = 0; i < loopClosure3D3D.size(); ++i)
        {
            Point3DFeaturePtr p1 = loopClosure3D3D.at(i).get<4>();
            Point3DFeaturePtr p2 = loopClosure3D3D.at(i).get<5>();

            p2->features2D().insert(p2->features2D().end(), p1->features2D().begin(), p1->features2D().end());
            std::sort(p2->features2D().begin(), p2->features2D().end());
            p2->features2D().erase(std::unique(p2->features2D().begin(), p2->features2D().end()), p2->features2D().end());

            std::vector<Point2DFeaturePtr> features2D = p2->features2D();
            for (size_t j = 0; j < features2D.size(); ++j)
            {
                features2D.at(j)->feature3D() = p2;
            }
        }

#ifdef VCHARGE_VIZ
        visualizeFrameFrameCorrespondences("loop-p-p", loopClosureFrameFrame);
#endif

        if (mVerbose)
        {
            std::cout << "# INFO: Done." << std::endl;
            std::cout << "# INFO: Running BA on odometry data... " << std::endl;
        }

        // perform BA to optimize intrinsics, extrinsics and scene points
        if (optimizeIntrinsics)
        {
            // perform BA to optimize intrinsics, extrinsics, odometry poses, and scene points
            optimize(CAMERA_INTRINSICS | CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D, true);
        }
        else
        {
            // perform BA to optimize extrinsics, odometry poses, and scene points
            optimize(CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D, true);
        }

        prune(PRUNE_BEHIND_CAMERA);

        double minError, maxError, avgError;
        size_t featureCount;

        reprojectionError(minError, maxError, avgError, featureCount, ODOMETRY);

        prune(PRUNE_BEHIND_CAMERA | PRUNE_FARAWAY | PRUNE_HIGH_REPROJ_ERR);

        if (mVerbose)
        {
            std::cout << "# INFO: Done." << std::endl;
            std::cout << "# INFO: Reprojection error after loop closures: avg = " << avgError
                      << " px | max = " << maxError << " px | count = " << featureCount << std::endl;
        }

#ifdef VCHARGE_VIZ
        visualize("opt3-BA-", ODOMETRY);

        visualizeExtrinsics("opt3-extrinsics");
#endif

        if (saveWorkingData)
        {
            for (int i = 0; i < mCameras.size(); ++i)
            {
                std::ostringstream oss;
                oss << "tmp_intrinsic_" << i << ".yaml";

                boost::filesystem::path intrinsicPath(dataDir);
                intrinsicPath /= oss.str();
                mCameras.at(i)->writeParameters(intrinsicPath.string());
            }

            boost::filesystem::path extrinsicPath(dataDir);
            extrinsicPath /= "tmp_extrinsic_3.txt";
            mExtrinsics.writeToFile(extrinsicPath.string());

            boost::filesystem::path graphPath(dataDir);
            graphPath /= "frames_3.sg";
            mGraph.writeToBinaryFile(graphPath.string());
        }
    }

    if (beginStage <= 4)
    {
        double zGround = 0.0;
        if (findAbsoluteGroundHeight(zGround))
        {
            if (mVerbose)
            {
                std::cout << "# INFO: Found ground plane: z = " << zGround << std::endl;
            }

            for (size_t i = 0; i < mCameras.size(); ++i)
            {
                Eigen::Matrix4d cameraPose = mExtrinsics.getGlobalCameraPose(i);
                cameraPose(2,3) -= zGround;

                mExtrinsics.setGlobalCameraPose(i, cameraPose);
            }
        }
        else
        {
            if (mVerbose)
            {
                std::cout << "# INFO: Did not find ground plane." << std::endl;
            }
        }
    }
}

void
CameraRigBA::setVerbose(bool verbose)
{
    mVerbose = verbose;
}

void
CameraRigBA::frameReprojectionError(const FramePtr& frame,
                                    const CameraConstPtr& camera,
                                    const Pose& T_cam_odo,
                                    double& minError, double& maxError, double& avgError,
                                    size_t& featureCount,
                                    int type) const
{
    minError = std::numeric_limits<double>::max();
    maxError = std::numeric_limits<double>::min();

    size_t count = 0;
    double totalError = 0.0;

    const std::vector<Point2DFeaturePtr>& features2D = frame->features2D();

    for (size_t i = 0; i < features2D.size(); ++i)
    {
        const Point2DFeatureConstPtr& feature2D = features2D.at(i);
        const Point3DFeatureConstPtr& feature3D = feature2D->feature3D();

        if (feature3D.get() == 0)
        {
            continue;
        }

        double error = 0.0;

        if (type == ODOMETRY)
        {
            error = reprojectionError(camera, feature3D->point(),
                                      T_cam_odo.rotation(),
                                      T_cam_odo.translation(),
                                      frame->odometry()->position(),
                                      frame->odometry()->attitude(),
                                      Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y));
        }
        else
        {
            error = camera->reprojectionError(feature3D->point(),
                                              frame->camera()->rotation(),
                                              frame->camera()->translation(),
                                              Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y));
        }

        if (minError > error)
        {
            minError = error;
        }
        if (maxError < error)
        {
            maxError = error;
        }
        totalError += error;
        ++count;
    }

    if (count == 0)
    {
        avgError = 0.0;
        minError = 0.0;
        maxError = 0.0;
        featureCount = count;

        return;
    }

    avgError = totalError / count;
    featureCount = count;
}

void
CameraRigBA::reprojectionError(double& minError, double& maxError,
                               double& avgError, size_t& featureCount,
                               int type) const
{
    minError = std::numeric_limits<double>::max();
    maxError = std::numeric_limits<double>::min();

    size_t count = 0;
    double totalError = 0.0;

    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        Pose T_cam_odo(mExtrinsics.getGlobalCameraPose(i));

        for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
        {
            const FrameSegment& segment = mGraph.frameSegments(i).at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                double frameMinError;
                double frameMaxError;
                double frameAvgError;
                size_t frameFeatureCount;

                frameReprojectionError(segment.at(k),
                                       mCameras.at(i),
                                       T_cam_odo,
                                       frameMinError, frameMaxError, frameAvgError, frameFeatureCount,
                                       type);

                if (minError > frameMinError)
                {
                    minError = frameMinError;
                }
                if (maxError < frameMaxError)
                {
                    maxError = frameMaxError;
                }
                totalError += frameAvgError * frameFeatureCount;
                count += frameFeatureCount;
            }
        }
    }

    if (count == 0)
    {
        avgError = 0.0;
        minError = 0.0;
        maxError = 0.0;
        featureCount = 0;

        return;
    }

    avgError = totalError / count;
    featureCount = count;
}

double
CameraRigBA::reprojectionError(const CameraConstPtr& camera,
                               const Eigen::Vector3d& P,
                               const Eigen::Quaterniond& cam_odo_q,
                               const Eigen::Vector3d& cam_odo_t,
                               const Eigen::Vector3d& odo_p,
                               const Eigen::Vector3d& odo_att,
                               const Eigen::Vector2d& observed_p) const
{
    Eigen::Quaterniond q_z_inv(cos(odo_att(0) / 2.0), 0.0, 0.0, -sin(odo_att(0) / 2.0));
    Eigen::Quaterniond q_y_inv(cos(odo_att(1) / 2.0), 0.0, -sin(odo_att(1) / 2.0), 0.0);
    Eigen::Quaterniond q_x_inv(cos(odo_att(2) / 2.0), -sin(odo_att(2) / 2.0), 0.0, 0.0);

    Eigen::Quaterniond q_world_odo = q_x_inv * q_y_inv * q_z_inv;
    Eigen::Quaterniond q_cam = cam_odo_q.conjugate() * q_world_odo;

    Eigen::Vector3d t_cam = - q_cam.toRotationMatrix() * odo_p - cam_odo_q.conjugate().toRotationMatrix() * cam_odo_t;

    return camera->reprojectionError(P, q_cam, t_cam, observed_p);
}

void
CameraRigBA::triangulateFeatures(FramePtr& frame1, FramePtr& frame2, FramePtr& frame3,
                                 const CameraConstPtr& camera,
                                 const Pose& T_cam_odo)
{
    // triangulate new feature correspondences seen in last 3 frames
    std::vector<std::vector<Point2DFeaturePtr> > featureCorrespondences;

    // use features that are seen in frames 0, 1, and 2
    find2D2DCorrespondences(frame3->features2D(), 3, featureCorrespondences);

//    if (mVerbose)
//    {
//        std::cout << "# INFO: Found " << featureCorrespondences.size() << " feature correspondences in last 3 frames." << std::endl;
//    }

    std::vector<cv::Point2f> ipoints[3];

    std::vector<std::vector<Point2DFeaturePtr> > untriFeatureCorrespondences;
    for (size_t i = 0; i < featureCorrespondences.size(); ++i)
    {
        std::vector<Point2DFeaturePtr>& fc = featureCorrespondences.at(i);

        Point2DFeaturePtr& f0 = fc.at(0);
        Point2DFeaturePtr& f1 = fc.at(1);
        Point2DFeaturePtr& f2 = fc.at(2);

        if (f0->feature3D().get() == 0 && f1->feature3D().get() == 0)
        {
            ipoints[0].push_back(f0->keypoint().pt);
            ipoints[1].push_back(f1->keypoint().pt);
            ipoints[2].push_back(f2->keypoint().pt);

            untriFeatureCorrespondences.push_back(fc);
        }

        if (f0->feature3D().get() != 0 && f1->feature3D().get() != 0)
        {
            f2->feature3D() = f1->feature3D();
            f2->feature3D()->features2D().push_back(f2);
            frame3->features3D().push_back(f2->feature3D());
        }
    }

//    if (mVerbose)
//    {
//        std::cout << "# INFO: Found " << untriFeatureCorrespondences.size() << " untriangulated feature correspondences." << std::endl;
//    }

    if (!untriFeatureCorrespondences.empty())
    {
        std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> > points3D;
        std::vector<size_t> indices;

        Eigen::Matrix4d H_odo_cam = T_cam_odo.pose().inverse();

        Eigen::Matrix4d H1 = H_odo_cam * frame1->odometry()->pose().inverse();
        Eigen::Matrix4d H2 = H_odo_cam * frame2->odometry()->pose().inverse();
        Eigen::Matrix4d H3 = H_odo_cam * frame3->odometry()->pose().inverse();

        tvt(camera,
            H1, ipoints[0],
            H2, ipoints[1],
            H3, ipoints[2],
            points3D, indices);

//        if (mVerbose)
//        {
//            std::cout << "# INFO: Triangulated " << points3D.size() << " new points." << std::endl;
//
//            if (!points3D.empty())
//            {
//                size_t count = 0;
//                double errorTotal = 0.0;
//                double errorMax = std::numeric_limits<double>::min();
//
//                for (size_t i = 0; i < points3D.size(); ++i)
//                {
//                    const cv::Point2f& feature2D = ipoints[0].at(indices.at(i));
//
//                    const Eigen::Vector3d& feature3D = points3D.at(i);
//
//                    OdometryReprojectionError reprojErr(cameraParameters,
//                                                        feature2D.x, feature2D.y);
//
//                    double residuals[2];
//                    reprojErr(T_cam_odo.rotationData(),
//                              T_cam_odo.translationData(),
//                              frame1->odometry()->positionData(),
//                              frame1->odometry()->attitudeData(),
//                              feature3D.data(), residuals);
//
//                    double error = hypot(residuals[0], residuals[1]);
//                    errorTotal += error;
//
//                    if (error > errorMax)
//                    {
//                        errorMax = error;
//                    }
//
//                    ++count;
//                }
//
//                double errorAvg = errorTotal / count;
//
//                std::cout << "# INFO: Reprojection error in frame n-2: avg = " << errorAvg
//                          << " px | max = " << errorMax << " px." << std::endl;
//
//                count = 0;
//                errorTotal = 0.0;
//                errorMax = std::numeric_limits<double>::min();
//
//                for (size_t i = 0; i < points3D.size(); ++i)
//                {
//                    const cv::Point2f& feature2D = ipoints[1].at(indices.at(i));
//
//                    const Eigen::Vector3d& feature3D = points3D.at(i);
//
//                    OdometryReprojectionError reprojErr(cameraParameters,
//                                                        feature2D.x, feature2D.y);
//
//                    double residuals[2];
//                    reprojErr(T_cam_odo.rotationData(),
//                              T_cam_odo.translationData(),
//                              frame2->odometry()->positionData(),
//                              frame2->odometry()->attitudeData(),
//                              feature3D.data(), residuals);
//
//                    double error = hypot(residuals[0], residuals[1]);
//                    errorTotal += error;
//
//                    if (error > errorMax)
//                    {
//                        errorMax = error;
//                    }
//
//                    ++count;
//                }
//
//                errorAvg = errorTotal / count;
//
//                std::cout << "# INFO: Reprojection error in frame n-1: avg = " << errorAvg
//                          << " px | max = " << errorMax << " px." << std::endl;
//
//                count = 0;
//                errorTotal = 0.0;
//                errorMax = std::numeric_limits<double>::min();
//
//                for (size_t i = 0; i < points3D.size(); ++i)
//                {
//                    const cv::Point2f& feature2D = ipoints[2].at(indices.at(i));
//
//                    const Eigen::Vector3d& feature3D = points3D.at(i);
//
//                    OdometryReprojectionError reprojErr(cameraParameters,
//                                                        feature2D.x, feature2D.y);
//
//                    double residuals[2];
//                    reprojErr(T_cam_odo.rotationData(),
//                              T_cam_odo.translationData(),
//                              frame3->odometry()->positionData(),
//                              frame3->odometry()->attitudeData(),
//                              feature3D.data(), residuals);
//
//                    double error = hypot(residuals[0], residuals[1]);
//                    errorTotal += error;
//
//                    if (error > errorMax)
//                    {
//                        errorMax = error;
//                    }
//
//                    ++count;
//                }
//
//                errorAvg = errorTotal / count;
//
//                std::cout << "# INFO: Reprojection error in frame n: avg = " << errorAvg
//                          << " px | max = " << errorMax << " px." << std::endl;
//            }
//        }

        for (size_t i = 0; i < points3D.size(); ++i)
        {
            Point3DFeaturePtr point3D(new Point3DFeature);

            point3D->point() = points3D.at(i);

            std::vector<Point2DFeaturePtr>& fc = untriFeatureCorrespondences.at(indices.at(i));

            for (int j = 0; j < 3; ++j)
            {
                Point2DFeaturePtr& pt = fc.at(j);

                point3D->features2D().push_back(pt);
                pt->feature3D() = point3D;
            }

            frame3->features3D().push_back(point3D);
        }
    }

//    if (mVerbose)
//        double minError, maxError, avgError;
//        size_t featureCount;
//
//        frameReprojectionError(frame3, cameraParameters, T_cam_odo, minError, maxError, avgError, featureCount);
//
//        std::cout << "# INFO: Frame reprojection error: min = " << minError << " | max = " << maxError << " | avg = " << avgError << std::endl;
//    }
}

void
CameraRigBA::find2D2DCorrespondences(const std::vector<Point2DFeaturePtr>& features,
                                     int nViews,
                                     std::vector<std::vector<Point2DFeaturePtr> >& correspondences) const
{
    // find feature correspondences across n views starting backward from
    // specified feature set in nth view
    if (nViews < 2)
    {
        return;
    }

    correspondences.reserve(features.size());

    for (size_t i = 0; i < features.size(); ++i)
    {
        Point2DFeaturePtr pt[nViews];

        pt[nViews - 1] = features.at(i);
        bool foundCorrespondences = true;

        for (int j = nViews - 1; j > 0; --j)
        {
            if (pt[j]->prevMatches().empty() || pt[j]->bestPrevMatchIdx() == -1)
            {
                foundCorrespondences = false;
                break;
            }

            pt[j - 1] = pt[j]->prevMatch();
        }

        if (!foundCorrespondences)
        {
            continue;
        }

        std::vector<Point2DFeaturePtr> correspondence(nViews);
        for (int j = 0; j < nViews; ++j)
        {
            correspondence.at(j) = pt[j];
        }

        correspondences.push_back(correspondence);
    }
}

void
CameraRigBA::buildVocTree(void)
{
    mVid2FidLUT.clear();

    if (mVerbose)
    {
        std::cout << "# INFO: Loading vocabulary... " << std::flush;
    }

    Surf64Vocabulary voc;
    voc.load("surf64.yml.gz");

    if (mVerbose)
    {
        std::cout << "Finished." << std::endl;
    }

    if (mVerbose)
    {
        std::cout << "# INFO: Width = " << voc.getBranchingFactor() << std::endl;
        std::cout << "# INFO: Depth = " << voc.getDepthLevels() << std::endl;
    }

    mDb.setVocabulary(voc);

    // build vocabulary tree
    std::vector<std::vector<std::vector<float> > > features;

    for (size_t cameraIdx = 0; cameraIdx < mCameras.size(); ++cameraIdx)
    {
        std::vector<FrameSegment>& segments = mGraph.frameSegments(cameraIdx);

        for (size_t segmentIdx = 0; segmentIdx < segments.size(); ++segmentIdx)
        {
            if (mVerbose)
            {
                std::cout << "# INFO: Computing bags-of-words for segment "
                          << segmentIdx + 1 << "/" << segments.size()
                          << " (cam " << cameraIdx << ")" << std::endl;
            }

            FrameSegment& segment = segments.at(segmentIdx);

            for (size_t frameIdx = 0; frameIdx < segment.size(); ++frameIdx)
            {
                FramePtr& frame = segment.at(frameIdx);

                FrameID fid;
                fid.cameraIdx = cameraIdx;
                fid.segmentIdx = segmentIdx;
                fid.frameIdx = frameIdx;
                mVid2FidLUT.push_back(fid);

                features.push_back(frameToBOW(frame));
            }
        }
    }

    for (size_t i = 0; i < features.size(); ++i)
    {
        mDb.add(features.at(i));
    }
}

std::vector<std::vector<float> >
CameraRigBA::frameToBOW(const FrameConstPtr& frame) const
{
    std::vector<std::vector<float> > bow;

    const std::vector<Point2DFeaturePtr>& features2D = frame->features2D();
    for (size_t i = 0; i < features2D.size(); ++i)
    {
        const Point2DFeatureConstPtr& feature2D = features2D.at(i);
        const cv::Mat& dtor = feature2D->descriptor();

        std::vector<float> w(dtor.cols);
        for (int j = 0; j < dtor.cols; ++j)
        {
            w.at(j) = dtor.at<float>(0,j);
        }

        bow.push_back(w);
    }

    return bow;
}

void
CameraRigBA::findLoopClosure3D3D(std::vector<boost::tuple<int, int, FramePtr, FramePtr> >& correspondencesFrameFrame,
                                 std::vector<Correspondence3D3D>& correspondences3D3D,
                                 double reprojErrorThresh)
{
    std::vector<Correspondence3D3D> corr3D3D[mCameras.size()];
    std::vector<boost::tuple<int, int, FramePtr, FramePtr> > corrFF[mCameras.size()];

    Glib::Thread* threads[mCameras.size()];
    for (int i = 0; i < mCameras.size(); ++i)
    {
        threads[i] = Glib::Thread::create(sigc::bind(sigc::mem_fun(*this, &CameraRigBA::findLoopClosure3D3DHelper),
                                                     i, &corrFF[i], &corr3D3D[i], reprojErrorThresh));
    }

    for (int i = 0; i < mCameras.size(); ++i)
    {
        threads[i]->join();

        correspondencesFrameFrame.insert(correspondencesFrameFrame.end(), corrFF[i].begin(), corrFF[i].end());
        correspondences3D3D.insert(correspondences3D3D.end(), corr3D3D[i].begin(), corr3D3D[i].end());
    }
}

void
CameraRigBA::findLoopClosure3D3DHelper(int cameraIdx,
                                       std::vector<boost::tuple<int, int, FramePtr, FramePtr> >* corrFF,
                                       std::vector<Correspondence3D3D>* corr3D3D,
                                       double reprojErrorThresh)
{
    reprojErrorThresh /= kNominalFocalLength;

    std::vector<Pose, Eigen::aligned_allocator<Pose> > T_cam_odo(mCameras.size());
    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        T_cam_odo.at(i) = Pose(mExtrinsics.getGlobalCameraPose(i));
    }

    // find closest matching images
    std::vector<FrameSegment>& segments = mGraph.frameSegments(cameraIdx);

    for (int segmentIdx = 0; segmentIdx < segments.size(); ++segmentIdx)
    {
        FrameSegment& segment = segments.at(segmentIdx);

        for (int frameIdx = 0; frameIdx < segment.size(); ++frameIdx)
        {
            FramePtr& frame1 = segment.at(frameIdx);

            DBoW2::QueryResults ret;
            mDb.query(frameToBOW(frame1), ret, kNearestImageMatches);

            std::vector<Correspondence3D3D> corr3D3DBest;
            FramePtr frame2Best;
            FrameID fidBest;
            int nInliersBest;
            for (size_t i = 0; i < ret.size(); ++i)
            {
                FrameID fid = mVid2FidLUT.at(ret.at(i).Id);

                if (fid.cameraIdx == cameraIdx &&
                    fid.segmentIdx == segmentIdx &&
                    abs(fid.frameIdx - frameIdx) < 20)
                {
                    continue;
                }

                FramePtr& frame2 = mGraph.frameSegments(fid.cameraIdx).at(fid.segmentIdx).at(fid.frameIdx);

                // mark 3D-3D correspondences between maps
                std::vector<cv::DMatch> matches = matchFeatures(frame1->features2D(), frame2->features2D());

                std::vector<std::pair<Point2DFeaturePtr, Point2DFeaturePtr> > corr2D2D;
                for (size_t j = 0; j < matches.size(); ++j)
                {
                    cv::DMatch& match = matches.at(j);

                    corr2D2D.push_back(std::make_pair(frame1->features2D().at(match.queryIdx),
                                                      frame2->features2D().at(match.trainIdx)));
                }

                if (corr2D2D.size() < kMinLoopCorrespondences2D2D)
                {
                    continue;
                }

                std::vector<cv::Point2f> points1, points2;
                points1.reserve(corr2D2D.size());
                points2.reserve(corr2D2D.size());

                for (size_t j = 0; j < corr2D2D.size(); ++j)
                {
                    Point2DFeaturePtr& pf1 = corr2D2D.at(j).first;
                    Point2DFeaturePtr& pf2 = corr2D2D.at(j).second;

                    cv::Point2f rectPt1, rectPt2;
                    rectifyImagePoint(mCameras.at(cameraIdx), pf1->keypoint().pt, rectPt1);
                    rectifyImagePoint(mCameras.at(fid.cameraIdx), pf2->keypoint().pt, rectPt2);

                    points1.push_back(rectPt1);
                    points2.push_back(rectPt2);
                }

                cv::Mat E, inlierMat;
                E = findEssentialMat(points1, points2, 1.0, cv::Point2d(0.0, 0.0), CV_FM_RANSAC, 0.99, reprojErrorThresh, 1000, inlierMat);

                if (cv::countNonZero(inlierMat) < kMinLoopCorrespondences2D2D)
                {
                    continue;
                }

                std::vector<Correspondence3D3D> corr3D3D;
                for (int j = 0; j < corr2D2D.size(); ++j)
                {
                    if (inlierMat.at<unsigned char>(0,j))
                    {
                        Point3DFeaturePtr& p1 = corr2D2D.at(j).first->feature3D();
                        Point3DFeaturePtr& p2 = corr2D2D.at(j).second->feature3D();

                        if (p1.get() == 0 || p2.get() == 0)
                        {
                            continue;
                        }

                        corr3D3D.push_back(boost::make_tuple(cameraIdx, fid.cameraIdx,
                                                             frame1, frame2,
                                                             p1, p2));
                    }
                }

                if (corr3D3D.size() > corr3D3DBest.size())
                {
                    corr3D3DBest = corr3D3D;
                    frame2Best = frame2;
                    fidBest = fid;
                    nInliersBest = cv::countNonZero(inlierMat);
                }
            }

            if (!corr3D3DBest.empty())
            {
                corrFF->push_back(boost::make_tuple(cameraIdx, cameraIdx, frame1, frame2Best));

                if (mVerbose)
                {
                    std::cout << "# INFO: Image match: " << cameraIdx << "," << segmentIdx << "," << frameIdx << " -> "
                              << fidBest.cameraIdx << "," << fidBest.segmentIdx << "," << fidBest.frameIdx << " with "
                              << nInliersBest << " 2D-2D correspondences and "
                              << corr3D3DBest.size() << " 3D-3D correspondences." << std::endl;
                }

                corr3D3D->insert(corr3D3D->end(), corr3D3DBest.begin(), corr3D3DBest.end());

#ifdef VCHARGE_VIZ
                visualize3D3DCorrespondences("loop-3d-3d", corr3D3DBest);
#endif
            }
        }
    }

    if (mVerbose)
    {
        std::cout << "# INFO: # loop 3D-3D correspondences for camera "
                  << cameraIdx << ": " << corr3D3D->size() << std::endl;
    }
}

std::vector<cv::DMatch>
CameraRigBA::matchFeatures(const std::vector<Point2DFeaturePtr>& features1,
                           const std::vector<Point2DFeaturePtr>& features2) const
{
    cv::BFMatcher descriptorMatcher(cv::NORM_L2, false);

    std::vector<size_t> indices1, indices2;
    cv::Mat dtor1 = buildDescriptorMat(features1, indices1);
    cv::Mat dtor2 = buildDescriptorMat(features2, indices2);

    std::vector<std::vector<cv::DMatch> > candidateFwdMatches;
    descriptorMatcher.knnMatch(dtor1, dtor2, candidateFwdMatches, 2);

    std::vector<std::vector<cv::DMatch> > candidateRevMatches;
    descriptorMatcher.knnMatch(dtor2, dtor1, candidateRevMatches, 2);

    std::vector<std::vector<cv::DMatch> > fwdMatches(candidateFwdMatches.size());
    for (size_t i = 0; i < candidateFwdMatches.size(); ++i)
    {
        std::vector<cv::DMatch>& match = candidateFwdMatches.at(i);

        if (match.size() < 2)
        {
            continue;
        }

        float distanceRatio = match.at(0).distance / match.at(1).distance;

        if (distanceRatio < kMaxDistanceRatio)
        {
            fwdMatches.at(i).push_back(match.at(0));
        }
    }

    std::vector<std::vector<cv::DMatch> > revMatches(candidateRevMatches.size());
    for (size_t i = 0; i < candidateRevMatches.size(); ++i)
    {
        std::vector<cv::DMatch>& match = candidateRevMatches.at(i);

        if (match.size() < 2)
        {
            continue;
        }

        float distanceRatio = match.at(0).distance / match.at(1).distance;

        if (distanceRatio < kMaxDistanceRatio)
        {
            revMatches.at(i).push_back(match.at(0));
        }
    }

    // cross-check
    std::vector<cv::DMatch> matches;
    for (size_t i = 0; i < fwdMatches.size(); ++i)
    {
        if (fwdMatches.at(i).empty())
        {
            continue;
        }

        cv::DMatch& fwdMatch = fwdMatches.at(i).at(0);

        if (revMatches.at(fwdMatch.trainIdx).empty())
        {
            continue;
        }

        cv::DMatch& revMatch = revMatches.at(fwdMatch.trainIdx).at(0);

        if (fwdMatch.queryIdx == revMatch.trainIdx &&
            fwdMatch.trainIdx == revMatch.queryIdx)
        {
            cv::DMatch match;
            match.queryIdx = indices1.at(fwdMatch.queryIdx);
            match.trainIdx = indices2.at(revMatch.queryIdx);

            matches.push_back(match);
        }
    }

    return matches;
}

void
CameraRigBA::findLocalInterMap2D2DCorrespondences(std::vector<Correspondence2D2D>& correspondences2D2D,
                                                  double reprojErrorThresh)
{
    std::map<uint64_t, std::vector<FramePtr> > pathMap;

    for (int cameraIdx = 0; cameraIdx < mCameras.size(); ++cameraIdx)
    {
        std::vector<FrameSegment>& segments = mGraph.frameSegments(cameraIdx);

        for (size_t segmentIdx = 0; segmentIdx < segments.size(); ++segmentIdx)
        {
            FrameSegment& segment = segments.at(segmentIdx);

            for (size_t frameIdx = 0; frameIdx < segment.size(); ++frameIdx)
            {
                FramePtr& frame = segment.at(frameIdx);

                std::vector<FramePtr>& frames = pathMap[frame->odometry()->timeStamp()];

                if (frames.empty())
                {
                    frames.resize(mCameras.size());
                }

                frames.at(cameraIdx) = frame;
            }
        }
    }

    std::list<FramePtr> windows[mCameras.size()];
    int pathNodeIdx = -1;
    for (std::map<uint64_t, std::vector<FramePtr> >::iterator it = pathMap.begin();
            it != pathMap.end(); ++it)
    {
        ++pathNodeIdx;

//        if (pathNodeIdx > 60)
//        {
//            return;
//        }

        std::vector<FramePtr>& frames = it->second;

        std::vector<FramePtr> localFrameMaps[mCameras.size()];
        for (int cameraIdx = 0; cameraIdx < mCameras.size(); ++cameraIdx)
        {
            std::list<FramePtr>& window = windows[cameraIdx];

            if (frames.at(cameraIdx).get() != 0)
            {
                windows[cameraIdx].push_front(frames.at(cameraIdx));
            }

            std::list<FramePtr>::iterator itWindow = windows[cameraIdx].begin();
            bool init = false;
            Eigen::Vector3d lastOdoPos;

            double dist = 0.0;
            while (itWindow != windows[cameraIdx].end() && dist < kLocalMapWindowDistance)
            {
                if (!init)
                {
                    lastOdoPos = (*itWindow)->odometry()->position();

                    init = true;

                    ++itWindow;

                    continue;
                }

                dist += ((*itWindow)->odometry()->position() - lastOdoPos).norm();

                lastOdoPos = (*itWindow)->odometry()->position();

                if (dist < kLocalMapWindowDistance)
                {
                    ++itWindow;
                }
            }

            if (itWindow != windows[cameraIdx].end())
            {
                windows[cameraIdx].erase(itWindow, windows[cameraIdx].end());
            }
        }

        if (mVerbose)
        {
            std::cout << "# INFO: Entering path node " << pathNodeIdx
                      << "/" << pathMap.size() - 1 << " [ ";

            for (int i = 0; i < mCameras.size(); ++i)
            {
                std::cout << windows[i].size() << " ";
            }
            std::cout << "]" << std::endl;
        }

        // project each map into other cameras
        for (int cameraIdx1 = 0; cameraIdx1 < mCameras.size(); ++cameraIdx1)
        {
            FramePtr& frame1 = frames.at(cameraIdx1);

            if (frame1.get() == 0)
            {
                continue;
            }

            if (frame1->features2D().empty())
            {
                continue;
            }

            Glib::Thread* threads[mCameras.size()];
            std::vector<Correspondence2D2D> subCorr2D2D[mCameras.size()];
            for (int cameraIdx2 = 0; cameraIdx2 < mCameras.size(); ++cameraIdx2)
            {
                if (cameraIdx1 == cameraIdx2)
                {
                    continue;
                }

                std::vector<FramePtr> window;
                window.insert(window.end(), windows[cameraIdx2].begin(), windows[cameraIdx2].end());

                threads[cameraIdx2] = Glib::Thread::create(sigc::bind(sigc::mem_fun(*this, &CameraRigBA::matchFrameToWindow),
                                                                      cameraIdx1, cameraIdx2, frame1, window,
                                                                      &subCorr2D2D[cameraIdx2],
                                                                      reprojErrorThresh));
            }

            for (int cameraIdx2 = 0; cameraIdx2 < mCameras.size(); ++cameraIdx2)
            {
                if (cameraIdx1 == cameraIdx2)
                {
                    continue;
                }

                threads[cameraIdx2]->join();

                correspondences2D2D.insert(correspondences2D2D.end(), subCorr2D2D[cameraIdx2].begin(), subCorr2D2D[cameraIdx2].end());
            }
        }
    }
}

void
CameraRigBA::matchFrameToWindow(int cameraIdx1, int cameraIdx2,
                                FramePtr& frame1,
                                std::vector<FramePtr>& window,
                                std::vector<Correspondence2D2D>* correspondences2D2D,
                                double reprojErrorThresh)
{
    if (window.empty())
    {
        return;
    }

    if (frame1->image().empty())
    {
        return;
    }

    Glib::Thread* threads[window.size()];
    std::vector<std::pair<Point2DFeaturePtr, Point2DFeaturePtr> > corr2D2D[window.size()];

    int windowIdx = -1;
    for (std::vector<FramePtr>::iterator itFrame2 = window.begin(); itFrame2 != window.end(); ++itFrame2)
    {
        ++windowIdx;
        FramePtr& frame2 = *itFrame2;

        threads[windowIdx] = Glib::Thread::create(sigc::bind(sigc::mem_fun(*this, &CameraRigBA::matchFrameToFrame),
                                                             cameraIdx1, cameraIdx2, frame1, frame2,
                                                             &corr2D2D[windowIdx],
                                                             reprojErrorThresh));
    }

    int windowIdxBest = -1;
    std::vector<std::pair<Point2DFeaturePtr, Point2DFeaturePtr> > corr2D2DBest;

    for (windowIdx = 0; windowIdx < window.size(); ++windowIdx)
    {
        threads[windowIdx]->join();

        if (corr2D2D[windowIdx].size() > corr2D2DBest.size())
        {
            windowIdxBest = windowIdx;
            corr2D2DBest = corr2D2D[windowIdx];
        }

//        if (mVerbose && !corr2D2D[windowIdx].empty())
//        {
//            std::cout << "# INFO:   cam " << cameraIdx1 << " - cam " << cameraIdx2
//                      << ": window " << windowIdx
//                      << " | " << corr2D2D[windowIdx].size() << " 2D-2D" << std::endl;
//        }
    }

    if (!corr2D2DBest.empty())
    {
        FramePtr& frame2 = window.at(windowIdxBest);

        for (size_t i = 0; i < corr2D2DBest.size(); ++i)
        {
            Point2DFeaturePtr& p1 = corr2D2DBest.at(i).first;
            Point2DFeaturePtr& p2 = corr2D2DBest.at(i).second;

            if (p1->feature3D().get() == 0 && p2->feature3D().get() == 0)
            {
                Eigen::Vector3d scenePoint;
                if (!triangulate3DPoint(p1, p2, scenePoint))
                {
                    continue;
                }

                Point3DFeaturePtr feature3D(new Point3DFeature);
                feature3D->point() = scenePoint;
                feature3D->features2D().push_back(p1);
                feature3D->features2D().push_back(p2);

                p1->feature3D() = feature3D;
                p2->feature3D() = feature3D;
            }
            else
            {
                if (p1->feature3D().get() == 0)
                {
                    Point3DFeaturePtr feature3D(new Point3DFeature);
                    feature3D->point() = p2->feature3D()->point();
                    feature3D->features2D().push_back(p1);

                    p1->feature3D() = feature3D;
                }

                if (p2->feature3D().get() == 0)
                {
                    Point3DFeaturePtr feature3D(new Point3DFeature);
                    feature3D->point() = p1->feature3D()->point();
                    feature3D->features2D().push_back(p2);

                    p2->feature3D() = feature3D;
                }
            }

            correspondences2D2D->push_back(corr2D2DBest.at(i));
        }

        if (mVerbose)
        {
            std::cout << "# INFO: Best: cam " << cameraIdx1 << " - cam " << cameraIdx2
                      << ": window " << windowIdxBest
                      << " | " << corr2D2DBest.size() << " 3D-3D" << std::endl;
        }

#ifdef VCHARGE_VIZ
        visualize3D3DCorrespondences("local-inter-3d-3d", corr2D2DBest);
#endif
    }
}

void
CameraRigBA::matchFrameToFrame(int cameraIdx1, int cameraIdx2,
                               FramePtr& frame1, FramePtr& frame2,
                               std::vector<Correspondence2D2D>* corr2D2D,
                               double reprojErrorThresh)
{
    std::vector<size_t> inliers2D2D;

    if (frame2.get() == 0)
    {
        return;
    }

    if (frame2->features2D().empty())
    {
        return;
    }

    if (frame2->image().empty())
    {
        return;
    }

    Pose T_odo_cam1(mExtrinsics.getGlobalCameraPose(cameraIdx1).inverse());
    Pose T_odo_cam2(mExtrinsics.getGlobalCameraPose(cameraIdx2).inverse());

    Eigen::Matrix4d H_cam1 = T_odo_cam1.pose() * frame1->odometry()->pose().inverse();
    Eigen::Matrix4d H_cam2 = T_odo_cam2.pose() * frame2->odometry()->pose().inverse();

    // compute average rotation between camera pair
    Eigen::JacobiSVD<Eigen::Matrix3d> svd(H_cam1.block<3,3>(0,0) +
                                          H_cam2.block<3,3>(0,0),
                                          Eigen::ComputeFullU | Eigen::ComputeFullV);

    Eigen::Matrix3d avgR = svd.matrixU() * svd.matrixV().transpose();

    Eigen::Matrix3d R1 = avgR * H_cam1.block<3,3>(0,0).transpose();
    Eigen::Matrix3d R2 = avgR * H_cam2.block<3,3>(0,0).transpose();

    cv::Mat R1_cv, R2_cv;
    cv::eigen2cv(R1, R1_cv);
    cv::eigen2cv(R2, R2_cv);

    cv::Mat mapX1, mapY1, mapX2, mapY2;
    if (mCameras.at(cameraIdx1)->modelType() == Camera::PINHOLE)
    {
        mCameras.at(cameraIdx1)->initUndistortRectifyMap(mapX1, mapY1,
                                                         -1.0f, -1.0f,
                                                         cv::Size(0, 0),
                                                         -1.0f, -1.0f, R1_cv);
    }
    else
    {
        mCameras.at(cameraIdx1)->initUndistortRectifyMap(mapX1, mapY1,
                                                         kNominalFocalLength, kNominalFocalLength,
                                                         cv::Size(0, 0),
                                                         -1.0f, -1.0f, R1_cv);
    }
    if (mCameras.at(cameraIdx2)->modelType() == Camera::PINHOLE)
    {
        mCameras.at(cameraIdx2)->initUndistortRectifyMap(mapX2, mapY2,
                                                         -1.0f, -1.0f,
                                                         cv::Size(0, 0),
                                                         -1.0f, -1.0f, R2_cv);
    }
    else
    {
        mCameras.at(cameraIdx2)->initUndistortRectifyMap(mapX2, mapY2,
                                                         kNominalFocalLength, kNominalFocalLength,
                                                         cv::Size(0, 0),
                                                         -1.0f, -1.0f, R2_cv);
    }

    cv::Mat rimg1, rimg2;
    cv::remap(frame1->image(), rimg1, mapX1, mapY1, cv::INTER_LINEAR);
    cv::remap(frame2->image(), rimg2, mapX2, mapY2, cv::INTER_LINEAR);

    cv::Mat rmask1, rmask2;
    if (!mCameras.at(cameraIdx1)->mask().empty())
    {
        cv::remap(mCameras.at(cameraIdx1)->mask(), rmask1, mapX1, mapY1, cv::INTER_LINEAR);
    }
    if (!mCameras.at(cameraIdx2)->mask().empty())
    {
        cv::remap(mCameras.at(cameraIdx2)->mask(), rmask2, mapX2, mapY2, cv::INTER_LINEAR);
    }

    cv::Ptr<SurfGPU> surf = SurfGPU::instance(200.0);

    std::vector<cv::KeyPoint> rkeypoints1, rkeypoints2;
    std::vector<cv::DMatch> rmatches;

    surf->match(rimg1, rkeypoints1, rmask1, rimg2, rkeypoints2, rmask2, rmatches);

    if (rmatches.size() < kMinInterCorrespondences2D2D)
    {
        return;
    }

    std::vector<cv::Point2f> rpoints1, rpoints2;
    for (size_t i = 0; i < rmatches.size(); ++i)
    {
        cv::DMatch& match = rmatches.at(i);

        rpoints1.push_back(rkeypoints1.at(match.queryIdx).pt);
        rpoints2.push_back(rkeypoints2.at(match.trainIdx).pt);
    }

    cv::Mat E, inlierMat;
    E = findEssentialMat(rpoints1, rpoints2, 1.0, cv::Point2d(0.0, 0.0), CV_FM_RANSAC, 0.99, reprojErrorThresh / kNominalFocalLength, 1000, inlierMat);

    if (cv::countNonZero(inlierMat) < kMinInterCorrespondences2D2D)
    {
        return;
    }

    if (0)
    {
        static boost::mutex rmutex;
        rmutex.lock();

        static int rcount = 0;
        std::ostringstream oss;
        oss << "r-inter-map-matches-" << cameraIdx1 << "-" << cameraIdx2 << "-" << rcount << ".png";

        std::vector<cv::DMatch> inlierRMatches;
        for (size_t i = 0; i < rmatches.size(); ++i)
        {
            if (inlierMat.at<unsigned char>(0, i) != 0)
            {
                inlierRMatches.push_back(rmatches.at(i));
            }
        }

        cv::Mat sketch;
        cv::drawMatches(rimg1, rkeypoints1, rimg2, rkeypoints2, inlierRMatches, sketch);

        cv::imwrite(oss.str(), sketch);

        oss.str(""); oss.clear();
        oss << "r" << cameraIdx1 << "img" << rcount << ".png";
        cv::imwrite(oss.str(), frame1->image());

        oss.str(""); oss.clear();
        oss << "r" << cameraIdx2 << "img" << rcount << ".png";
        cv::imwrite(oss.str(), frame2->image());

        ++rcount;

        rmutex.unlock();
    }

    Eigen::Matrix4d H1 = Eigen::Matrix4d::Identity();
    H1.block<3,3>(0,0) = R1;
    Eigen::Matrix4d H_rcam1 = H1 * H_cam1;

    Eigen::Matrix4d H2 = Eigen::Matrix4d::Identity();
    H2.block<3,3>(0,0) = R2;
    Eigen::Matrix4d H_rcam2 = H2 * H_cam2;

    std::vector<std::pair<Point2DFeaturePtr, Point2DFeaturePtr> > candidateCorr2D2D(rmatches.size());
    for (size_t i = 0; i < rmatches.size(); ++i)
    {
        if (inlierMat.at<unsigned char>(0, i) == 0)
        {
            continue;
        }

        cv::Point2f& rp = rpoints1.at(i);

        // compute corresponding image point in original image in camera 1
        Eigen::Vector3d ray;
        ray(0) = (rp.x - mCameras.at(cameraIdx1)->imageWidth() / 2.0) / kNominalFocalLength;
        ray(1) = (rp.y - mCameras.at(cameraIdx1)->imageHeight() / 2.0) / kNominalFocalLength;
        ray(2) = 1.0;

        ray = R1.transpose() * ray;

        Eigen::Vector2d ep;
        mCameras.at(cameraIdx1)->spaceToPlane(ray, ep);

        // find closest image point to computed image point
        float kpDistMin = std::numeric_limits<float>::max();
        Point2DFeaturePtr p2DBest;
        for (size_t j = 0; j < frame1->features2D().size(); ++j)
        {
            Point2DFeaturePtr& p2D = frame1->features2D().at(j);

            float kpDist = hypot(ep(0) - p2D->keypoint().pt.x,
                                 ep(1) - p2D->keypoint().pt.y);
            if (kpDist < kpDistMin)
            {
                kpDistMin = kpDist;
                p2DBest = p2D;
            }
        }

        if (kpDistMin < reprojErrorThresh)
        {
            candidateCorr2D2D.at(i).first = p2DBest;
        }

        rp = rpoints2.at(i);

        // compute corresponding image point in original image in camera 2
        ray(0) = (rp.x - mCameras.at(cameraIdx2)->imageWidth() / 2.0) / kNominalFocalLength;
        ray(1) = (rp.y - mCameras.at(cameraIdx2)->imageHeight() / 2.0) / kNominalFocalLength;
        ray(2) = 1.0;

        ray = R2.transpose() * ray;

        mCameras.at(cameraIdx2)->spaceToPlane(ray, ep);

        // find closest image point to computed image point
        kpDistMin = std::numeric_limits<float>::max();
        for (size_t j = 0; j < frame2->features2D().size(); ++j)
        {
            Point2DFeaturePtr& p2D = frame2->features2D().at(j);

            float kpDist = hypot(ep(0) - p2D->keypoint().pt.x,
                                 ep(1) - p2D->keypoint().pt.y);
            if (kpDist < kpDistMin)
            {
                kpDistMin = kpDist;
                p2DBest = p2D;
            }
        }

        if (kpDistMin < reprojErrorThresh)
        {
            candidateCorr2D2D.at(i).second = p2DBest;
        }
    }

    if (0)
    {
        std::vector<cv::KeyPoint> keypoints1, keypoints2;
        std::vector<cv::DMatch> matches;
        for (size_t i = 0; i < candidateCorr2D2D.size(); ++i)
        {
            Point2DFeaturePtr& p1 = candidateCorr2D2D.at(i).first;
            Point2DFeaturePtr& p2 = candidateCorr2D2D.at(i).second;

            if (p1.get() == 0 || p2.get() == 0)
            {
                continue;
            }

            keypoints1.push_back(p1->keypoint());
            keypoints2.push_back(p2->keypoint());

            cv::DMatch match;
            match.queryIdx = keypoints1.size() - 1;
            match.trainIdx = keypoints2.size() - 1;
            matches.push_back(match);
        }

        static boost::mutex mutex;
        mutex.lock();

        static int count = 0;
        std::ostringstream oss;
        oss << "ur-inter-map-matches-" << cameraIdx1 << "-" << cameraIdx2 << "-" << count << ".png";

        cv::Mat sketch;
        cv::drawMatches(frame1->image(), keypoints1, frame2->image(), keypoints2, matches, sketch);

        cv::imwrite(oss.str(), sketch);

        oss.str(""); oss.clear();
        oss << "ur" << cameraIdx1 << "img" << count << ".png";
        cv::imwrite(oss.str(), frame1->image());

        oss.str(""); oss.clear();
        oss << "ur" << cameraIdx2 << "img" << count << ".png";
        cv::imwrite(oss.str(), frame2->image());

        ++count;

        mutex.unlock();
    }

    for (size_t i = 0; i < candidateCorr2D2D.size(); ++i)
    {
        Point2DFeaturePtr& p1 = candidateCorr2D2D.at(i).first;
        Point2DFeaturePtr& p2 = candidateCorr2D2D.at(i).second;

        if (p1.get() == 0 || p2.get() == 0)
        {
            continue;
        }

        corr2D2D->push_back(candidateCorr2D2D.at(i));
    }
}

cv::Mat
CameraRigBA::buildDescriptorMat(const std::vector<Point2DFeaturePtr>& features,
                                std::vector<size_t>& indices) const
{
    for (size_t i = 0; i < features.size(); ++i)
    {
         if (features.at(i)->feature3D().get() != 0)
         {
             indices.push_back(i);
         }
    }

    cv::Mat dtor(indices.size(), features.at(0)->descriptor().cols, features.at(0)->descriptor().type());

    for (size_t i = 0; i < indices.size(); ++i)
    {
         features.at(indices.at(i))->descriptor().copyTo(dtor.row(i));
    }

    return dtor;
}

bool
CameraRigBA::project3DPoint(const CameraConstPtr& camera,
                            const Eigen::Matrix4d& H,
                            const Eigen::Vector4d& src, Eigen::Vector3d& dst) const
{
    // transform point from world frame to camera frame
    Eigen::Vector3d P = H.block<3,3>(0,0) * (src.block<3,1>(0,0) / src(3)) + H.block<3,1>(0,3);

    // check if point is behind camera
    if (P(2) < 0.0)
    {
        return false;
    }

    Eigen::Vector2d p;
    camera->spaceToPlane(P, p);
    dst << p, 1.0;

    return true;
}

void
CameraRigBA::rectifyImagePoint(const CameraConstPtr& camera,
                               const cv::Point2f& src, cv::Point2f& dst) const
{
    Eigen::Vector3d P;

    camera->liftProjective(Eigen::Vector2d(src.x, src.y), P);

    P /= P(2);

    dst.x = P(0);
    dst.y = P(1);
}

void
CameraRigBA::rectifyImagePoint(const CameraConstPtr& camera,
                               const Eigen::Vector2d& src, Eigen::Vector2d& dst) const
{
    Eigen::Vector3d P;

    camera->liftProjective(src, P);

    P /= P(2);

    dst = P.block<2,1>(0,0);
}

void
CameraRigBA::rectifyImagePoints(const CameraConstPtr& camera,
                                const std::vector<cv::Point2f>& src,
                                std::vector<cv::Point2f>& dst) const
{
    dst.resize(src.size());

    for (size_t i = 0; i < src.size(); ++i)
    {
        const cv::Point2f& p = src.at(i);

        Eigen::Vector3d P;
        camera->liftProjective(Eigen::Vector2d(p.x, p.y), P);

        P /= P(2);

        dst.at(i) = cv::Point2f(P(0), P(1));
    }
}

void
CameraRigBA::tvt(const CameraConstPtr& camera,
                 const Eigen::Matrix4d& H1,
                 const std::vector<cv::Point2f>& imagePoints1,
                 const Eigen::Matrix4d& H2,
                 const std::vector<cv::Point2f>& imagePoints2,
                 const Eigen::Matrix4d& H3,
                 const std::vector<cv::Point2f>& imagePoints3,
                 std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> >& points3D,
                 std::vector<size_t>& inliers) const
{
    Eigen::Matrix<double, 3, 4> P1 = H1.block<3,4>(0,0);
    Eigen::Matrix<double, 3, 4> P2 = H2.block<3,4>(0,0);
    Eigen::Matrix<double, 3, 4> P3 = H3.block<3,4>(0,0);

    Eigen::Matrix4d H12 = H2 * H1.inverse();
    Eigen::Matrix4d H23 = H3 * H2.inverse();
    Eigen::Matrix4d H13 = H3 * H1.inverse();

    Eigen::Matrix3d E12 = skew(Eigen::Vector3d(H12.block<3,1>(0,3))) * H12.block<3,3>(0,0);
    Eigen::Matrix3d E23 = skew(Eigen::Vector3d(H23.block<3,1>(0,3))) * H23.block<3,3>(0,0);
    Eigen::Matrix3d E13 = skew(Eigen::Vector3d(H13.block<3,1>(0,3))) * H13.block<3,3>(0,0);

    // linear triangulation
    for (size_t i = 0; i < imagePoints1.size(); ++i)
    {
        const cv::Point2f& p1_cv = imagePoints1.at(i);
        const cv::Point2f& p2_cv = imagePoints2.at(i);
        const cv::Point2f& p3_cv = imagePoints3.at(i);

        cv::Point2f rect_p1_cv, rect_p2_cv, rect_p3_cv;
        rectifyImagePoint(camera, p1_cv, rect_p1_cv);
        rectifyImagePoint(camera, p2_cv, rect_p2_cv);
        rectifyImagePoint(camera, p3_cv, rect_p3_cv);

        Eigen::Matrix4d J;
        J.row(0) = P2.row(2) * rect_p2_cv.x - P2.row(0);
        J.row(1) = P2.row(2) * rect_p2_cv.y - P2.row(1);
        J.row(2) = P3.row(2) * rect_p3_cv.x - P3.row(0);
        J.row(3) = P3.row(2) * rect_p3_cv.y - P3.row(1);

        Eigen::JacobiSVD<Eigen::MatrixXd> svd(J, Eigen::ComputeThinU | Eigen::ComputeThinV);
        Eigen::Vector4d scenePoint = svd.matrixV().block<4,1>(0,3);

        scenePoint /= scenePoint(3);

        // validate scene point
        Eigen::Vector3d p1, p2, p3;
        if (!project3DPoint(camera, H1, scenePoint, p1))
        {
            continue;
        }
        if (!project3DPoint(camera, H2, scenePoint, p2))
        {
            continue;
        }
        if (!project3DPoint(camera, H3, scenePoint, p3))
        {
            continue;
        }

        points3D.push_back(scenePoint.block<3,1>(0,0));
        inliers.push_back(i);
    }
}

bool
CameraRigBA::triangulate3DPoint(const Point2DFeatureConstPtr& p1,
                                const Point2DFeatureConstPtr& p2,
                                Eigen::Vector3d& scenePoint,
                                double reprojErrorThresh) const
{
    const CameraPtr& cam1 = mCameras.at(p1->frame()->cameraId());
    const CameraPtr& cam2 = mCameras.at(p2->frame()->cameraId());

    const cv::Point2f& pt1 = p1->keypoint().pt;
    const cv::Point2f& pt2 = p2->keypoint().pt;

    // projective rays
    Eigen::Vector3d ray1, ray2;
    cam1->liftProjective(Eigen::Vector2d(pt1.x, pt1.y), ray1);
    cam2->liftProjective(Eigen::Vector2d(pt2.x, pt2.y), ray2);

    ray1 = mExtrinsics.getGlobalCameraPose(p1->frame()->cameraId()).block<3,3>(0,0) * ray1;
    ray2 = mExtrinsics.getGlobalCameraPose(p2->frame()->cameraId()).block<3,3>(0,0) * ray2;

    ray1.normalize();
    ray2.normalize();

    // compute Plucker line correspondence
    Eigen::Matrix<double,6,1> l1, l2;
    l1 << ray1, mExtrinsics.getGlobalCameraPose(p1->frame()->cameraId()).block<3,1>(0,3).cross(ray1);
    l2 << ray2, mExtrinsics.getGlobalCameraPose(p2->frame()->cameraId()).block<3,1>(0,3).cross(ray2);

    Eigen::Vector3d q1 = l1.head(3);
    Eigen::Vector3d q1p = l1.tail(3);

    Eigen::Vector3d q2 = l2.head(3);
    Eigen::Vector3d q2p = l2.tail(3);

    Eigen::Vector3d q1xq1p = q1.cross(q1p);

    Eigen::Matrix4d H;
    H = p2->frame()->odometry()->pose().inverse() *
        p1->frame()->odometry()->pose();

    Eigen::MatrixXd A(3,2);
    A.col(0) = H.block<3,3>(0,0) * q1;
    A.col(1) = -q2;

    Eigen::Vector3d b = q2.cross(q2p) - H.block<3,3>(0,0) * q1xq1p - H.block<3,1>(0,3);

    Eigen::Vector2d gamma = A.jacobiSvd(Eigen::ComputeThinU | Eigen::ComputeThinV).solve(b);

    scenePoint = q1xq1p + gamma(0) * q1;
    scenePoint = p1->frame()->odometry()->pose().block<3,3>(0,0) * scenePoint + p1->frame()->odometry()->pose().block<3,1>(0,3);

    Pose T_cam1_odo(mExtrinsics.getGlobalCameraPose(p1->frame()->cameraId()));
    double e1 = reprojectionError(cam1, scenePoint, T_cam1_odo.rotation(), T_cam1_odo.translation(),
                                  p1->frame()->odometry()->position(), p1->frame()->odometry()->attitude(),
                                  Eigen::Vector2d(pt1.x, pt1.y));

    Pose T_cam2_odo(mExtrinsics.getGlobalCameraPose(p2->frame()->cameraId()));
    double e2 = reprojectionError(cam2, scenePoint, T_cam2_odo.rotation(), T_cam2_odo.translation(),
                                  p2->frame()->odometry()->position(), p2->frame()->odometry()->attitude(),
                                  Eigen::Vector2d(pt2.x, pt2.y));

    if (e1 > reprojErrorThresh || e2 > reprojErrorThresh)
    {
        return false;
    }

    return true;
}

void
CameraRigBA::prune(int flags, int poseType)
{
    // prune points that are too far away or behind a camera
    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        Pose T_cam_odo(mExtrinsics.getGlobalCameraPose(i));

        Eigen::Matrix4d H_odo_cam = T_cam_odo.pose().inverse();

        for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
        {
            FrameSegment& segment = mGraph.frameSegments(i).at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                FramePtr& frame = segment.at(k);

                std::vector<Point2DFeaturePtr>& features2D = frame->features2D();

                Eigen::Matrix4d H_cam = Eigen::Matrix4d::Identity();
                if (frame->camera().get() != 0)
                {
                    H_cam = frame->camera()->pose();
                }

                Eigen::Matrix4d H_odo_inv = frame->odometry()->pose().inverse();

                for (size_t l = 0; l < features2D.size(); ++l)
                {
                    Point2DFeaturePtr& pf = features2D.at(l);

                    if (pf->feature3D().get() == 0)
                    {
                        continue;
                    }

                    Eigen::Vector4d P;
                    P << pf->feature3D()->point(), 1.0;

                    Eigen::Vector4d P_cam;
                    if (poseType == CAMERA)
                    {
                        P_cam = H_cam * P;
                    }
                    else
                    {
                        P_cam = (H_odo_cam * H_odo_inv) * P;
                    }

                    bool prune = false;

                    if ((flags & PRUNE_BEHIND_CAMERA) == PRUNE_BEHIND_CAMERA &&
                        P_cam(2) < 0.0)
                    {
                        prune = true;
                    }

                    if ((flags & PRUNE_FARAWAY) == PRUNE_FARAWAY &&
                        P_cam.block<3,1>(0,0).norm() > kMaxPoint3DDistance)
                    {
                        prune = true;
                    }

                    if ((flags & PRUNE_HIGH_REPROJ_ERR) == PRUNE_HIGH_REPROJ_ERR)
                    {
                        double error = 0.0;

                        if (poseType == CAMERA)
                        {
                            error = mCameras.at(i)->reprojectionError(pf->feature3D()->point(),
                                                                      frame->camera()->rotation(),
                                                                      frame->camera()->translation(),
                                                                      Eigen::Vector2d(pf->keypoint().pt.x, pf->keypoint().pt.y));
                        }
                        else
                        {
                            error = reprojectionError(mCameras.at(i),
                                                      pf->feature3D()->point(),
                                                      T_cam_odo.rotation(),
                                                      T_cam_odo.translation(),
                                                      frame->odometry()->position(),
                                                      frame->odometry()->attitude(),
                                                      Eigen::Vector2d(pf->keypoint().pt.x, pf->keypoint().pt.y));
                        }

                        if (error > kMaxReprojErr)
                        {
                            prune = true;
                        }
                    }

                    if (prune)
                    {
                        // delete entire feature track
                        std::vector<Point2DFeaturePtr> features2D = pf->feature3D()->features2D();

                        for (size_t m = 0; m < features2D.size(); ++m)
                        {
                            features2D.at(m)->feature3D() = Point3DFeaturePtr();
                        }
                    }
                }
            }
        }
    }
}

void
CameraRigBA::optimize(int flags, bool optimizeZ, int nIterations)
{
    // find number of points seen by more than 1 camera
    boost::unordered_set<Point3DFeature*> scenePointSet;
    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        const std::vector<FrameSegment>& segments = mGraph.frameSegments(i);
        for (size_t j = 0; j < segments.size(); ++j)
        {
            const FrameSegment& segment = segments.at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                const std::vector<Point2DFeaturePtr>& features2D = segment.at(k)->features2D();

                for (size_t l = 0; l < features2D.size(); ++l)
                {
                    if (features2D.at(l)->feature3D().get() == 0)
                    {
                        continue;
                    }

                    scenePointSet.insert(features2D.at(l)->feature3D().get());
                }
            }
        }
    }

    size_t nPoints = 0;
    size_t nPointsMultipleCams = 0;
    for (boost::unordered_set<Point3DFeature*>::iterator it = scenePointSet.begin();
         it != scenePointSet.end(); ++it)
    {
        if (seenByMultipleCameras((*it)->features2D()))
        {
            nPointsMultipleCams += (*it)->features2D().size();
        }

        nPoints += (*it)->features2D().size();
    }

    double weightM = static_cast<double>(nPoints) / static_cast<double>(nPointsMultipleCams);

    ceres::Problem problem;

    ceres::Solver::Options options;
    options.linear_solver_type = ceres::SPARSE_NORMAL_CHOLESKY;
    options.max_num_iterations = nIterations;
    options.num_threads = 8;
    options.num_linear_solver_threads = 8;

    // intrinsics
    std::vector<double> intrinsicParams[mCameras.size()];

    for (int i = 0; i < mCameras.size(); ++i)
    {
        mCameras.at(i)->writeParameters(intrinsicParams[i]);
    }

    // extrinsics
    std::vector<Pose, Eigen::aligned_allocator<Pose> > T_cam_odo(mCameras.size());
    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        T_cam_odo.at(i) = Pose(mExtrinsics.getGlobalCameraPose(i));
    }

    boost::dynamic_bitset<> optimizeExtrinsics(mCameras.size());

    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
        {
            for (size_t k = 0; k < mGraph.frameSegments(i).at(j).size(); ++k)
            {
                FramePtr& frame = mGraph.frameSegments(i).at(j).at(k);

                std::vector<Point2DFeaturePtr>& features2D = frame->features2D();

                for (size_t l = 0; l < features2D.size(); ++l)
                {
                    Point2DFeaturePtr& feature2D = features2D.at(l);
                    Point3DFeaturePtr& feature3D = feature2D->feature3D();

                    if (feature3D.get() == 0)
                    {
                        continue;
                    }

                    optimizeExtrinsics[i] = 1;

                    double weight = 1.0;
                    if (seenByMultipleCameras(feature3D->features2D()))
                    {
                        weight = weightM;
                    }

                    ceres::LossFunction* lossFunction = new ceres::ScaledLoss(new ceres::CauchyLoss(1.0), weight, ceres::TAKE_OWNERSHIP);

                    ceres::CostFunction* costFunction;
                    switch (flags)
                    {
                    case CAMERA_ODOMETRY_EXTRINSICS | POINT_3D:
                    {
                        costFunction
                            = CostFunctionFactory::instance()->generateCostFunction(mCameras.at(i),
                                                                                    frame->odometry()->position(),
                                                                                    frame->odometry()->attitude(),
                                                                                    Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y),
                                                                                    flags,
                                                                                    optimizeZ);

                        problem.AddResidualBlock(costFunction, lossFunction,
                                                 T_cam_odo.at(i).rotationData(),
                                                 T_cam_odo.at(i).translationData(),
                                                 feature3D->pointData());

                        break;
                    }
                    case CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_3D_EXTRINSICS | POINT_3D:
                    case CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D:
                    {
                        costFunction
                            = CostFunctionFactory::instance()->generateCostFunction(mCameras.at(i),
                                                                                    Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y),
                                                                                    flags,
                                                                                    optimizeZ);

                        problem.AddResidualBlock(costFunction, lossFunction,
                                                 T_cam_odo.at(i).rotationData(),
                                                 T_cam_odo.at(i).translationData(),
                                                 frame->odometry()->positionData(),
                                                 frame->odometry()->attitudeData(),
                                                 feature3D->pointData());

                        break;
                    }
                    case CAMERA_INTRINSICS | CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_3D_EXTRINSICS | POINT_3D:
                    case CAMERA_INTRINSICS | CAMERA_ODOMETRY_EXTRINSICS | ODOMETRY_6D_EXTRINSICS | POINT_3D:
                    {
                        costFunction
                            = CostFunctionFactory::instance()->generateCostFunction(mCameras.at(i),
                                                                                    Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y),
                                                                                    flags,
                                                                                    optimizeZ);

                        problem.AddResidualBlock(costFunction, lossFunction,
                                                 intrinsicParams[i].data(),
                                                 T_cam_odo.at(i).rotationData(),
                                                 T_cam_odo.at(i).translationData(),
                                                 frame->odometry()->positionData(),
                                                 frame->odometry()->attitudeData(),
                                                 feature3D->pointData());

                        break;
                    }
                    case CAMERA_EXTRINSICS | POINT_3D:
                    {
                        costFunction
                            = CostFunctionFactory::instance()->generateCostFunction(mCameras.at(i),
                                                                                    Eigen::Vector2d(feature2D->keypoint().pt.x, feature2D->keypoint().pt.y),
                                                                                    flags);

                        problem.AddResidualBlock(costFunction, lossFunction,
                                                 frame->camera()->rotationData(),
                                                 frame->camera()->translationData(),
                                                 feature3D->pointData());

                        break;
                    }
                    }
                }

                if (flags & CAMERA_EXTRINSICS)
                {
                    ceres::LocalParameterization* quaternionParameterization =
                        new EigenQuaternionParameterization;

                    problem.SetParameterization(frame->camera()->rotationData(), quaternionParameterization);
                }
            }
        }
    }

    if (flags & CAMERA_ODOMETRY_EXTRINSICS)
    {
        for (size_t i = 0; i < mCameras.size(); ++i)
        {
            if (optimizeExtrinsics[i])
            {
                ceres::LocalParameterization* quaternionParameterization =
                    new EigenQuaternionParameterization;

                problem.SetParameterization(T_cam_odo.at(i).rotationData(), quaternionParameterization);
            }
        }
    }

    ceres::Solver::Summary summary;
    ceres::Solve(options, &problem, &summary);

    if (mVerbose)
    {
        std::cout << summary.BriefReport() << std::endl;
    }

    bool computeCovariances = false;
    if (flags & ODOMETRY_6D_EXTRINSICS)
    {
        computeCovariances = true;
    }
    if (flags & POINT_3D)
    {
        computeCovariances = true;
    }

    if (0)
//    if (computeCovariances)
    {
        if (mVerbose)
        {
            std::cout << "# INFO: Computing covariances... " << std::flush;
        }

        // compute covariances
        ceres::Covariance::Options covOptions;
        covOptions.num_threads = 8;
        covOptions.algorithm_type = ceres::SPARSE_QR;
        covOptions.apply_loss_function = true;

        ceres::Covariance covariance(covOptions);

        std::vector<std::pair<const double*, const double*> > covarianceBlocks;

        if (flags & ODOMETRY_6D_EXTRINSICS)
        {
            for (size_t i = 0; i < mCameras.size(); ++i)
            {
                covarianceBlocks.push_back(std::make_pair(T_cam_odo.at(i).rotationData(),
                                                          T_cam_odo.at(i).rotationData()));
                covarianceBlocks.push_back(std::make_pair(T_cam_odo.at(i).rotationData(),
                                                          T_cam_odo.at(i).translationData()));
                covarianceBlocks.push_back(std::make_pair(T_cam_odo.at(i).translationData(),
                                                          T_cam_odo.at(i).translationData()));
            }
        }

        if (flags & POINT_3D)
        {
            for (boost::unordered_set<Point3DFeature*>::iterator it = scenePointSet.begin();
                    it != scenePointSet.end(); ++it)
            {
                covarianceBlocks.push_back(std::make_pair((*it)->pointData(),
                                                          (*it)->pointData()));
            }
        }

        if (covariance.Compute(covarianceBlocks, &problem))
        {
            if (flags & ODOMETRY_6D_EXTRINSICS)
            {
                for (size_t i = 0; i < mCameras.size(); ++i)
                {
                    double covariance_rr[4*4];
                    double covariance_rt[4*3];
                    double covariance_tt[3*3];

                    covariance.GetCovarianceBlock(T_cam_odo.at(i).rotationData(),
                                                  T_cam_odo.at(i).rotationData(),
                                                  covariance_rr);
                    covariance.GetCovarianceBlock(T_cam_odo.at(i).rotationData(),
                                                  T_cam_odo.at(i).translationData(),
                                                  covariance_rt);
                    covariance.GetCovarianceBlock(T_cam_odo.at(i).translationData(),
                                                  T_cam_odo.at(i).translationData(),
                                                  covariance_tt);

                    memcpy(T_cam_odo.at(i).covariance().block<4,4>(0,0).data(), covariance_rr, sizeof(double) * 16);
                    memcpy(T_cam_odo.at(i).covariance().block<4,3>(0,4).data(), covariance_rt, sizeof(double) * 12);
                    memcpy(T_cam_odo.at(i).covariance().block<3,3>(4,4).data(), covariance_tt, sizeof(double) * 9);
                    T_cam_odo.at(i).covariance().block<3,4>(4,0) = T_cam_odo.at(i).covariance().block<4,3>(0,4).transpose();
                }
            }

            if (flags & POINT_3D)
            {
                for (boost::unordered_set<Point3DFeature*>::iterator it = scenePointSet.begin();
                        it != scenePointSet.end(); ++it)
                {
                    double covariance_PP[3*3];

                    covariance.GetCovarianceBlock((*it)->pointData(),
                                                  (*it)->pointData(),
                                                  covariance_PP);

                    memcpy((*it)->pointCovarianceData(), covariance_PP, sizeof(double) * 9);
                }
            }

            if (mVerbose)
            {
                std::cout << "Finished." << std::endl;
            }
        }
        else
        {
            std::cout << std::endl << "# ERROR: Ceres was unable to compute the covariances." << std::endl;
        }
    }

    if (flags & CAMERA_ODOMETRY_EXTRINSICS)
    {
        for (size_t i = 0; i < mCameras.size(); ++i)
        {
            mExtrinsics.setGlobalCameraPose(i, T_cam_odo.at(i).pose());
        }
    }

    if (flags & CAMERA_INTRINSICS)
    {
        for (int i = 0; i < mCameras.size(); ++i)
        {
            mCameras.at(i)->readParameters(intrinsicParams[i]);
        }
    }
}

bool
CameraRigBA::seenByMultipleCameras(const std::vector<Point2DFeaturePtr>& features2D) const
{
    if (features2D.size() <= 1)
    {
        return false;
    }

    int firstCameraId = features2D.front()->frame()->cameraId();
    std::vector<Point2DFeaturePtr>::const_iterator it = features2D.begin() + 1;

    while (it != features2D.end())
    {
        if ((*it)->frame()->cameraId() != firstCameraId)
        {
            return true;
        }

        ++it;
    }

    return false;
}

bool
CameraRigBA::findAbsoluteGroundHeight(double& zGround) const
{
    return false;
}

#ifdef VCHARGE_VIZ
void
CameraRigBA::visualize(const std::string& overlayPrefix, int type)
{
    boost::unordered_set<Odometry*> odometrySet;

    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        std::ostringstream oss;
        oss << overlayPrefix << i + 1;

        std::vector<OdometryPtr> odometry;
        std::vector<PosePtr> cameras;
        std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> > scenePoints;

        boost::unordered_set<Point3DFeature*> scenePointSet;

        std::vector<FrameSegment>& segments = mGraph.frameSegments(i);
        for (size_t j = 0; j < segments.size(); ++j)
        {
            FrameSegment& segment = segments.at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                if (type == ODOMETRY)
                {
                    odometry.push_back(segment.at(k)->odometry());
                    odometrySet.insert(segment.at(k)->odometry().get());
                }
                else
                {
                    cameras.push_back(segment.at(k)->camera());
                }

                std::vector<Point2DFeaturePtr>& features2D = segment.at(k)->features2D();

                for (size_t l = 0; l < features2D.size(); ++l)
                {
                    if (features2D.at(l)->feature3D().get() == 0)
                    {
                        continue;
                    }

                    if (features2D.at(l)->feature3D()->point().norm() < 1000.0)
                    {
                        scenePointSet.insert(features2D.at(l)->feature3D().get());
                    }
                }
            }
        }

        vcharge::GLOverlayExtended overlay(oss.str(), VCharge::COORDINATE_FRAME_LOCAL);

        Eigen::Vector3d origin;
        if (type == ODOMETRY)
        {
            origin << odometry.front()->x(), odometry.front()->y(), 0.0;
        }
        else
        {
            origin << cameras.front()->translation()(0),
                      cameras.front()->translation()(1),
                      cameras.front()->translation()(2);
        }

        // visualize camera poses and 3D scene points
        overlay.clear();
        overlay.setOrigin(origin(0), origin(1), origin(2));
        overlay.pointSize(2.0f);
        overlay.lineWidth(1.0f);

        if (type == ODOMETRY)
        {
            for (size_t j = 0; j < odometry.size(); ++j)
            {
                OdometryPtr& odo = odometry.at(j);

                Eigen::Matrix4d H_odo = odo->pose();
                Eigen::Matrix4d H_cam = H_odo * mExtrinsics.getGlobalCameraPose(i);

                double xBound = 0.1;
                double yBound = 0.1;
                double zFar = 0.2;

                std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> > frustum;
                frustum.push_back(Eigen::Vector3d(0.0, 0.0, 0.0));
                frustum.push_back(Eigen::Vector3d(-xBound, -yBound, zFar));
                frustum.push_back(Eigen::Vector3d(xBound, -yBound, zFar));
                frustum.push_back(Eigen::Vector3d(xBound, yBound, zFar));
                frustum.push_back(Eigen::Vector3d(-xBound, yBound, zFar));

                for (size_t k = 0; k < frustum.size(); ++k)
                {
                    frustum.at(k) = transformPoint(H_cam, frustum.at(k)) - origin;
                }

                overlay.color4f(1.0f, 1.0f, 1.0f, 1.0f);
                overlay.begin(VCharge::LINES);

                for (int k = 1; k < 5; ++k)
                {
                    overlay.vertex3f(frustum.at(0)(0), frustum.at(0)(1), frustum.at(0)(2));
                    overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
                }

                overlay.end();

                switch (i)
                {
                case vcharge::CAMERA_FRONT:
                    overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
                    break;
                case vcharge::CAMERA_LEFT:
                    overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
                    break;
                case vcharge::CAMERA_REAR:
                    overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
                    break;
                case vcharge::CAMERA_RIGHT:
                    overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
                    break;
                default:
                    overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
                }

                overlay.begin(VCharge::POLYGON);

                for (int k = 1; k < 5; ++k)
                {
                    overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
                }

                overlay.end();
            }

            overlay.lineWidth(1.0f);
            overlay.pointSize(2.0f);
        }
        else
        {
            for (size_t j = 0; j < cameras.size(); ++j)
            {
                PosePtr& camera = cameras.at(j);

                Eigen::Matrix4d H_cam = camera->pose().inverse();

                double xBound = 0.1;
                double yBound = 0.1;
                double zFar = 0.2;

                std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> > frustum;
                frustum.push_back(Eigen::Vector3d(0.0, 0.0, 0.0));
                frustum.push_back(Eigen::Vector3d(-xBound, -yBound, zFar));
                frustum.push_back(Eigen::Vector3d(xBound, -yBound, zFar));
                frustum.push_back(Eigen::Vector3d(xBound, yBound, zFar));
                frustum.push_back(Eigen::Vector3d(-xBound, yBound, zFar));

                for (size_t k = 0; k < frustum.size(); ++k)
                {
                    frustum.at(k) = transformPoint(H_cam, frustum.at(k)) - origin;
                }

                overlay.color4f(1.0f, 1.0f, 1.0f, 1.0f);
                overlay.begin(VCharge::LINES);

                for (int k = 1; k < 5; ++k)
                {
                    overlay.vertex3f(frustum.at(0)(0), frustum.at(0)(1), frustum.at(0)(2));
                    overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
                }

                overlay.end();

                switch (i)
                {
                case vcharge::CAMERA_FRONT:
                    overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
                    break;
                case vcharge::CAMERA_LEFT:
                    overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
                    break;
                case vcharge::CAMERA_REAR:
                    overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
                    break;
                case vcharge::CAMERA_RIGHT:
                    overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
                    break;
                default:
                    overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
                }

                overlay.begin(VCharge::POLYGON);

                for (int k = 1; k < 5; ++k)
                {
                    overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
                }

                overlay.end();
            }
        }

        // draw 3D scene points
        switch (i)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
        }

        overlay.begin(VCharge::POINTS);

        for (boost::unordered_set<Point3DFeature*>::iterator it = scenePointSet.begin();
                 it != scenePointSet.end(); ++it)
        {
            Eigen::Vector3d p = (*it)->point() - origin;

            overlay.vertex3f(p(0), p(1), p(2));
        }

        overlay.end();

        overlay.publish();

        usleep(50000);
    }

    std::ostringstream oss;
    oss << overlayPrefix << "odo";

    vcharge::GLOverlayExtended overlay(oss.str(), VCharge::COORDINATE_FRAME_LOCAL);

    Eigen::Vector3d origin((*odometrySet.begin())->x(), (*odometrySet.begin())->y(), 0.0);

    overlay.clear();
    overlay.setOrigin(origin(0), origin(1), origin(2));
    overlay.lineWidth(1.0f);
    overlay.color3f(0.7f, 0.7f, 0.7f);

    double w_2 = 0.05;
    double l_2 = 0.1;

    double vertices[4][3] = {{-l_2, -w_2, 0.0},
                             {l_2, -w_2, 0.0},
                             {l_2, w_2, 0.0},
                             {-l_2, w_2, 0.0}};

    if (type == ODOMETRY)
    {
        for (boost::unordered_set<Odometry*>::iterator it = odometrySet.begin();
                it != odometrySet.end(); ++it)
        {
            Odometry* odo = *it;

            Eigen::Matrix4d H = odo->pose();

            overlay.begin(VCharge::LINE_LOOP);

            for (int i = 0; i < 4; ++i)
            {
                Eigen::Vector3d p;
                p << vertices[i][0], vertices[i][1], vertices[i][2];

                p = transformPoint(H, p) - origin;

                overlay.vertex3f(p(0), p(1), p(2));
            }

            overlay.end();

            Eigen::Vector3d p0(0.0, 0.0, 0.0);
            Eigen::Vector3d p1(l_2, 0.0, 0.0);

            p0 = transformPoint(H, p0) - origin;
            p1 = transformPoint(H, p1) - origin;

            overlay.begin(VCharge::LINES);

            overlay.vertex3f(p0(0), p0(1), p0(2));
            overlay.vertex3f(p1(0), p1(1), p1(2));

            overlay.end();
        }
    }

    overlay.publish();
}

void
CameraRigBA::visualizeExtrinsics(const std::string& overlayName)
{
    vcharge::GLOverlayExtended overlay(overlayName, VCharge::COORDINATE_FRAME_GLOBAL);

    // visualize extrinsics
    overlay.clear();
    overlay.lineWidth(1.0f);

    // x-axis
    overlay.color4f(1.0f, 0.0f, 0.0f, 1.0f);
    overlay.begin(VCharge::LINES);
    overlay.vertex3f(0.0f, 0.0f, 0.0f);
    overlay.vertex3f(0.3f, 0.0f, 0.0f);
    overlay.end();

    // y-axis
    overlay.color4f(0.0f, 1.0f, 0.0f, 1.0f);
    overlay.begin(VCharge::LINES);
    overlay.vertex3f(0.0f, 0.0f, 0.0f);
    overlay.vertex3f(0.0f, 0.3f, 0.0f);
    overlay.end();

    // z-axis
    overlay.color4f(0.0f, 0.0f, 1.0f, 1.0f);
    overlay.begin(VCharge::LINES);
    overlay.vertex3f(0.0f, 0.0f, 0.0f);
    overlay.vertex3f(0.0f, 0.0f, 0.3f);
    overlay.end();

    double z_ref = 0.0;
    for (int i = 0; i < mCameras.size(); ++i)
    {
        Eigen::Matrix4d H_cam = mExtrinsics.getGlobalCameraPose(i);

        z_ref += H_cam(2,3);
    }
    z_ref /= mCameras.size();

    for (int i = 0; i < mCameras.size(); ++i)
    {
        Eigen::Matrix4d H_cam = mExtrinsics.getGlobalCameraPose(i);

        double xBound = 0.1;
        double yBound = 0.1;
        double zFar = 0.2;

        std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> > frustum;
        frustum.push_back(Eigen::Vector3d(0.0, 0.0, 0.0));
        frustum.push_back(Eigen::Vector3d(-xBound, -yBound, zFar));
        frustum.push_back(Eigen::Vector3d(xBound, -yBound, zFar));
        frustum.push_back(Eigen::Vector3d(xBound, yBound, zFar));
        frustum.push_back(Eigen::Vector3d(-xBound, yBound, zFar));

        for (size_t k = 0; k < frustum.size(); ++k)
        {
            frustum.at(k) = transformPoint(H_cam, frustum.at(k));
            frustum.at(k)(2) -= z_ref;
        }

        overlay.color4f(1.0f, 1.0f, 1.0f, 1.0f);
        overlay.begin(VCharge::LINES);

        for (int k = 1; k < 5; ++k)
        {
            overlay.vertex3f(frustum.at(0)(0), frustum.at(0)(1), frustum.at(0)(2));
            overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
        }

        overlay.end();

        switch (i)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
        }

        overlay.begin(VCharge::POLYGON);

        for (int k = 1; k < 5; ++k)
        {
            overlay.vertex3f(frustum.at(k)(0), frustum.at(k)(1), frustum.at(k)(2));
        }

        overlay.end();
    }

    overlay.publish();
}

void
CameraRigBA::visualizeFrameFrameCorrespondences(const std::string& overlayName,
                                                const std::vector<boost::tuple<int, int, FramePtr, FramePtr> >& correspondencesFrameFrame) const
{
    vcharge::GLOverlayExtended overlay(overlayName, VCharge::COORDINATE_FRAME_GLOBAL);

    // visualize camera poses and 3D scene points
    overlay.clear();
    overlay.lineWidth(2.0f);

    for (size_t i = 0; i < correspondencesFrameFrame.size(); ++i)
    {
        int cameraIdx1 = correspondencesFrameFrame.at(i).get<0>();
        int cameraIdx2 = correspondencesFrameFrame.at(i).get<1>();
        const FramePtr& frame1 = correspondencesFrameFrame.at(i).get<2>();
        const FramePtr& frame2 = correspondencesFrameFrame.at(i).get<3>();

        const Eigen::Matrix4d& H_cam_odo1 = mExtrinsics.getGlobalCameraPose(cameraIdx1);
        const Eigen::Matrix4d& H_cam_odo2 = mExtrinsics.getGlobalCameraPose(cameraIdx2);

        Eigen::Matrix4d H_cam1 = frame1->odometry()->pose() * H_cam_odo1;
        Eigen::Matrix4d H_cam2 = frame2->odometry()->pose() * H_cam_odo2;

        overlay.begin(VCharge::LINES);

        switch (cameraIdx1)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
        }

        overlay.vertex3d(H_cam1(0,3), H_cam1(1,3), H_cam1(2,3));

        switch (cameraIdx2)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.5f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.5f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.5f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.5f);
        }

        overlay.vertex3d(H_cam2(0,3), H_cam2(1,3), H_cam2(2,3));
        overlay.end();
    }

    overlay.publish();
}

void
CameraRigBA::visualize3D3DCorrespondences(const std::string& overlayName,
                                          const std::vector<Correspondence3D3D>& correspondences) const
{
    vcharge::GLOverlayExtended overlay(overlayName, VCharge::COORDINATE_FRAME_GLOBAL);

    // visualize 3D-3D correspondences
    overlay.clear();
    overlay.lineWidth(1.0f);

    overlay.begin(VCharge::LINES);

    for (size_t i = 0; i < correspondences.size(); ++i)
    {
        int cameraIdx1 = correspondences.at(i).get<0>();
        int cameraIdx2 = correspondences.at(i).get<1>();

        const FramePtr& frame1 = correspondences.at(i).get<2>();
        const FramePtr& frame2 = correspondences.at(i).get<3>();

        Eigen::Vector3d p1 = correspondences.at(i).get<4>()->point();
        Eigen::Vector3d p2 = correspondences.at(i).get<5>()->point();

        const Eigen::Matrix4d& H_cam_odo1 = mExtrinsics.getGlobalCameraPose(cameraIdx1);
        const Eigen::Matrix4d& H_cam_odo2 = mExtrinsics.getGlobalCameraPose(cameraIdx2);

        Eigen::Matrix4d H_cam1 = frame1->odometry()->pose() * H_cam_odo1;
        Eigen::Matrix4d H_cam2 = frame2->odometry()->pose() * H_cam_odo2;

        switch (cameraIdx1)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.3f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.3f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.3f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.3f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.3f);
        }

        overlay.vertex3d(H_cam1(0,3), H_cam1(1,3), H_cam1(2,3));
        overlay.vertex3d(p1(0), p1(1), p1(2));

        overlay.color3f(1.0f, 1.0f, 1.0f);

        overlay.vertex3d(p1(0), p1(1), p1(2));
        overlay.vertex3d(p2(0), p2(1), p2(2));

        switch (cameraIdx2)
        {
        case vcharge::CAMERA_FRONT:
            overlay.color4f(1.0f, 0.0f, 0.0f, 0.7f);
            break;
        case vcharge::CAMERA_LEFT:
            overlay.color4f(0.0f, 1.0f, 0.0f, 0.7f);
            break;
        case vcharge::CAMERA_REAR:
            overlay.color4f(0.0f, 1.0f, 1.0f, 0.7f);
            break;
        case vcharge::CAMERA_RIGHT:
            overlay.color4f(1.0f, 1.0f, 0.0f, 0.7f);
            break;
        default:
            overlay.color4f(1.0f, 1.0f, 1.0f, 0.7f);
        }

        overlay.vertex3d(p2(0), p2(1), p2(2));
        overlay.vertex3d(H_cam2(0,3), H_cam2(1,3), H_cam2(2,3));
    }

    overlay.end();

    overlay.publish();
}

void
CameraRigBA::visualize3D3DCorrespondences(const std::string& overlayName,
                                          const std::vector<Correspondence2D2D>& correspondences2D2D) const
{
    std::vector<Correspondence3D3D> correspondences3D3D;
    correspondences3D3D.reserve(correspondences2D2D.size());

    for (size_t i = 0; i < correspondences2D2D.size(); ++i)
    {
        const Point2DFeaturePtr& p1 = correspondences2D2D.at(i).first;
        const Point2DFeaturePtr& p2 = correspondences2D2D.at(i).second;

        if (p1->feature3D().get() == 0 || p2->feature3D().get() == 0)
        {
            continue;
        }

        correspondences3D3D.push_back(boost::make_tuple(p1->frame()->cameraId(), p2->frame()->cameraId(),
                                                        p1->frame(), p2->frame(),
                                                        p1->feature3D(), p2->feature3D()));
    }

    visualize3D3DCorrespondences(overlayName, correspondences3D3D);
}

void
CameraRigBA::visualizeGroundPoints(const std::vector<Eigen::Vector3d, Eigen::aligned_allocator<Eigen::Vector3d> >& points) const
{
    vcharge::GLOverlayExtended overlay("ground-pts", VCharge::COORDINATE_FRAME_GLOBAL);

    // visualize 3D-3D correspondences
    overlay.clear();
    overlay.pointSize(3.0f);
    overlay.color3f(0.0f, 1.0f, 0.0f);

    overlay.begin(VCharge::POINTS);

    for (size_t i = 0; i < points.size(); ++i)
    {
        const Eigen::Vector3d& p = points.at(i);

        overlay.vertex3d(p(0), p(1), p(2));
    }

    overlay.end();

    overlay.publish();
}

#endif

bool
CameraRigBA::validateGraph(void) const
{
    bool valid = true;

    for (size_t i = 0; i < mCameras.size(); ++i)
    {
        Pose T_cam_odo(mExtrinsics.getGlobalCameraPose(i));

        for (size_t j = 0; j < mGraph.frameSegments(i).size(); ++j)
        {
            const FrameSegment& segment = mGraph.frameSegments(i).at(j);

            for (size_t k = 0; k < segment.size(); ++k)
            {
                const FramePtr& frame = segment.at(k);

                const std::vector<Point2DFeaturePtr>& features2D = frame->features2D();

                for (size_t i = 0; i < features2D.size(); ++i)
                {
                    const Point2DFeatureConstPtr& feature2D = features2D.at(i);
                    const Point3DFeatureConstPtr& feature3D = feature2D->feature3D();

                    if (feature2D->frame().get() != frame.get())
                    {
                        std::cout << "# WARNING: Container frame and parent frame do not match for 2D feature." << std::endl;
                        valid = false;
                    }

                    if (feature3D.get() == 0)
                    {
                        continue;
                    }
                }
            }
        }
    }

    return valid;
}

}
